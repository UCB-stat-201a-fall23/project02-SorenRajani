{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4f2e3179-b4ae-4243-92eb-ed8d86df7c10",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "## Project No2 - Concentration Theorems\n",
    "\n",
    "When writing code, we recommend you to be as modular as possible. For example, if you are running multiple experiments for different choices of parameters, it may be convenient to write a function that does one experiment and then make multiple calls to the same function. Follow the _do not repeat yourself_ rule when writing code! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "130e891e-23be-4df7-8209-9c95c8ec6852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['font.size'] = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c3daa1-9650-44c3-a364-1e7ebd3a9d37",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 0. Setup \n",
    "\n",
    "We will consider $X_1, X_2, \\ldots, X_n$ a sequence of i.i.d. random variables with mean $\\mathbb{E}[X_1] = \\mu$. We define the sample mean as \n",
    "$$\n",
    "\\bar X_n = \\frac{1}{n} \\sum_{i=1}^n X_i\n",
    "$$\n",
    "The objective of this project is to study the behavior of $\\bar X_n$ as $n \\rightarrow \\infty$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11dc797-24b8-4f93-9ef3-fb3d3b2d28aa",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 1. Law of Large Numbers\n",
    "\n",
    "Here we are going to focus in the convergence in probability and almost surely of the sample mean of i.i.d. random variables. Remember that a sequence of random variables $Y_1, Y_2, \\ldots$ converges in probability to a random variable $Y$ if for every $\\epsilon > 0$ we have \n",
    "$$\n",
    "\\lim_{n \\rightarrow \\infty} \\mathbb P \\left( | Y_n - Y | > \\epsilon \\right) = 0\n",
    "$$\n",
    "The **weak law of large numbers** states \n",
    "\n",
    "> The sample mean $\\bar X_n$ converges in probability to $\\mu$.\n",
    "\n",
    "The **strong law of large numbers** states a more strict sense in which this convergence happens\n",
    "\n",
    "> The sample mean $\\bar X_n$ converges almost surely to $\\mu$. \n",
    "\n",
    "Let's see how these two behave numerically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cfeaa1-d1be-4cc2-9769-3f2fcdf2d82f",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### 1.1. Gaussian case\n",
    "\n",
    "Consider the simple case where $X_i \\sim N(0,1)$. Compute the sample mean for increasing values of $n$ and show how the sample mean converges to $0$. To do this, you can simply show a plot of the sample mean as a function of $n$. \n",
    "\n",
    "***Tip:*** You can compute the sample mean of a normal random variable by simply doing\n",
    "```python\n",
    "n = 100\n",
    "one_sample_mean = np.mean(np.random.normal(loc=0.0, scale=1.0, size=n))\n",
    "```\n",
    "The same works if you want to compute a total of `n_sim` sample means:\n",
    "```python\n",
    "n = 100\n",
    "n_sim = 500\n",
    "many_sample_means = np.mean(np.random.normal(loc=0.0, scale=1.0, size=(n, n_sim)), axis=0)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5926ff-b9b2-4019-a481-4c8e7343a2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca4ed9c-95c5-4520-b0c7-1268e2916701",
   "metadata": {},
   "source": [
    "#### 1.2. Beta distribution\n",
    "\n",
    "Now, repeat the same experiment but for a Beta distribution with parameters of your choice. Remember that the mean of a Beta distribution with parameters $a$ and $b$ is $a / (a+b)$. \n",
    "\n",
    "For this section, it is important that you compute the sample mean as you increase the value of $n$ *for the same realization of the random variables $X_i$*. To do this, first sample all the values of $X_1, X_2, \\ldots, X_{n_{max}}$ just one time and then compute the partial averages $\\bar X_n$ for different values of $n \\leq n_{max}$. \n",
    "\n",
    "Make this plot is logarithmic scale for both axes. This is how your solution should look like. If you kernel dies when running the simulations, try reducing the total number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f2ee7b-25e1-48d6-9af5-8dc2fe3bf307",
   "metadata": {},
   "source": [
    "<img src=\"solution_1_2.png\" alt=\"Solution\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a822844b-7040-438a-becb-ab45caaa7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO DO CELL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4a167b-adbd-490a-be09-b92b2375b27c",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "**To Discuss:** When doing this calculations, what time of convergence are we studying? almost surely or in probability? \n",
    "\n",
    "**To Discuss:** Can you identify the general trend in the previous plot? Can you give meaning the low peaks in the plot? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785678cf-b4c7-42f4-8e6d-77dfbc9125cd",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### 1.3. Compute the probability\n",
    "\n",
    "Now, instead of evaluating the value of $\\bar X_n$, compute the probability $\\mathbb P \\left( | \\bar X_n | > \\epsilon \\right)$ for different values of $n$ and $\\epsilon$ and plot the result as a function of $n$. What do you observe? See the next item for an example of the solution for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694bd1cc-7e16-4881-9f36-658eeaca7a4f",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### 1.4. Concentration bounds\n",
    "\n",
    "Repeat the same plot that in the last item and show how it compares with the upper bound obtain from using Chebyshev's inequality\n",
    "$$\n",
    "\\mathbb P \\left( | \\bar X_n | > \\epsilon \\right) \\leq \\frac{1}{n \\epsilon^2}\n",
    "$$\n",
    "and the Chernoff bound\n",
    "$$\n",
    "\\mathbb P \\left( | \\bar X_n | > \\epsilon \\right) \\leq 2 \\exp \\left( - \\frac{\\epsilon^2 n}{2} \\right)\n",
    "$$\n",
    "At the end of the day, you should obtain something like this\n",
    "\n",
    "<img src=\"solution_1_34.png\" alt=\"Solution\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755260c8-4ec3-49ed-a783-99629dd51cb8",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### 2. Central Limit theorem \n",
    "\n",
    "Let's first state one more time the Central Limit Theorem\n",
    "\n",
    "> Consider $X_1, X_2, \\ldots, X_n$ a sequence of i.i.d. random variables with mean $\\mathbb{E}[X_1] = \\mu$ and finite variance $\\mathbb{V}ar[X_1] = \\sigma^2$. If call \n",
    "$$\n",
    "\\bar X_n = \\frac{1}{n} \\sum_{i=1}^n X_i\n",
    "$$\n",
    "the sample mean, then \n",
    "$$\n",
    "\\sqrt{n} \\frac{\\bar X_n - \\mu}{\\sigma}\n",
    "$$\n",
    "converges in distribution to the standard Normal distribution $N(0,1)$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09afbf2f-a800-4ccd-a82c-e2a4b01f8005",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### 2.1. Continuous case\n",
    "\n",
    "Pick a continuous random variable with bounded second moment and show that the central limit theorem holds numerically. Sample the sample mean $\\bar X_n$ for various numbers of $n$ and show that $\\sqrt{n}(\\bar X_n - \\mu) / \\sigma$ converges to the standard normal distribution.\n",
    "\n",
    "This is how the solution should look like for the mean of Beta distributions and a total of 10000 simulations. \n",
    "\n",
    "<img src=\"solution_2_1.png\" alt=\"Solution\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b63c3c-5cd0-4090-910b-06ded98c5497",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### 2.2. Discrete case\n",
    "\n",
    "Repeat the experiment but now with a discrete random variable. When plotting the histogram, be careful on how you define the bins. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec41f67-40be-4f3c-ad5b-0e85979d6689",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "#### 2.3. [Optional] Distribution without second moment\n",
    "\n",
    "Consider now a distribution with defined mean without finite second moment. In principle, the central limit theorem does not apply for this case. Generate a simulation of the sample mean for a distribution with these properties and try to understand the behavior of the scaled sample mean as $n$ increases. You may find interesting the reading about [stable distributions](https://en.wikipedia.org/wiki/Stable_distribution)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d944ef54-38c2-46dc-a74f-2383625f9021",
   "metadata": {
    "user_expressions": []
   },
   "source": [
    "### Contribution statement\n",
    "\n",
    "Please include a list of the students you work with along this project (including yourself). Include both names and GitHub usernames of the people you collaborated with. Maximum of three persons per group. \n",
    "- Member No1: \n",
    "- Member No2: \n",
    "- Member No3:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
